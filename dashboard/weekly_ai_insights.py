import os, json
import boto3
import pandas as pd
from datetime import datetime
from openai import OpenAI

# Initialize clients
athena = boto3.client("athena", region_name=os.getenv("AWS_REGION"))
s3 = boto3.client("s3", region_name=os.getenv("AWS_REGION"))
openai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

ATHENA_DB = "codesense360_db"
S3_OUTPUT = "s3://codesense360-data/athena-results/"
S3_BUCKET = "codesense360-data"

def run_query(query):
    """Run Athena query and return DataFrame."""
    response = athena.start_query_execution(
        QueryString=query,
        QueryExecutionContext={"Database": ATHENA_DB},
        ResultConfiguration={"OutputLocation": S3_OUTPUT},
    )
    exec_id = response["QueryExecutionId"]

    # Wait for completion
    while True:
        res = athena.get_query_execution(QueryExecutionId=exec_id)
        state = res["QueryExecution"]["Status"]["State"]
        if state in ["SUCCEEDED", "FAILED", "CANCELLED"]:
            break

    if state != "SUCCEEDED":
        raise Exception(f"Athena query failed: {state}")

    result = athena.get_query_results(QueryExecutionId=exec_id)
    cols = [col["Label"] for col in result["ResultSet"]["ResultSetMetadata"]["ColumnInfo"]]
    rows = [r["Data"] for r in result["ResultSet"]["Rows"][1:]]
    data = [[c.get("VarCharValue", "") for c in row] for row in rows]
    return pd.DataFrame(data, columns=cols)

def generate_summary():
    commits_df = run_query("""
        SELECT author_login, COUNT(*) AS commits
        FROM commits_processed
        WHERE author_login IS NOT NULL
        GROUP BY author_login
        ORDER BY commits DESC
        LIMIT 10;
    """)

    pr_df = run_query("""
        SELECT author, total_prs, merged_prs, avg_review_time_hours
        FROM author_pr_summary
        ORDER BY merged_prs DESC
        LIMIT 10;
    """)

    cicd_df = run_query("""
        SELECT conclusion, COUNT(*) AS total
        FROM workflow_runs_processed
        GROUP BY conclusion;
    """)

    prompt = f"""
    You are an engineering analytics assistant.
    Summarize weekly developer performance trends using:
    - Commits: {commits_df.to_dict(orient='records')}
    - Pull Requests: {pr_df.to_dict(orient='records')}
    - CI/CD runs: {cicd_df.to_dict(orient='records')}
    Use emojis üìàüìâ and produce a short summary under 150 words.
    """

    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=300,
        temperature=0.6,
    )

    insights = response.choices[0].message.content
    print("‚úÖ Weekly AI Insights Generated:\n", insights)
    return insights

import requests

def post_to_teams(insights_text):
    """Send insights to Microsoft Teams using Adaptive Card format."""
    webhook_url = os.getenv("TEAMS_WEBHOOK_URL")
    if not webhook_url:
        print("‚ö†Ô∏è No Teams webhook configured. Skipping post.")
        return

    # Build Adaptive Card payload
    payload = {
        "@type": "MessageCard",
        "@context": "https://schema.org/extensions",
        "themeColor": "0078D7",
        "summary": "CodeSense360 Weekly AI Insights",
        "sections": [
            {
                "activityTitle": "üß† **CodeSense360 Weekly AI Insights**",
                "activitySubtitle": f"üìÖ Week of {datetime.utcnow().strftime('%B %d, %Y')}",
                "facts": [
                    {"name": "üìà Data Source", "value": "AWS Athena + S3"},
                    {"name": "ü§ñ Generated by", "value": "OpenAI GPT-4"},
                    {"name": "‚òÅÔ∏è Region", "value": os.getenv("AWS_REGION", "us-east-2")},
                ],
                "text": f"**Insight Summary:**\n\n{insights_text}",
            }
        ],
        "potentialAction": [
            {
                "@type": "OpenUri",
                "name": "üîé View in Streamlit Dashboard",
                "targets": [{"os": "default", "uri": "https://your-streamlit-app-url"}],
            },
            {
                "@type": "OpenUri",
                "name": "üìÇ View S3 Folder",
                "targets": [{"os": "default", "uri": "https://s3.console.aws.amazon.com/s3/buckets/codesense360-data/weekly_insights"}],
            },
        ],
    }

    try:
        response = requests.post(webhook_url, json=payload)
        if response.status_code == 200:
            print("‚úÖ Posted rich adaptive card to Microsoft Teams.")
        else:
            print(f"‚ö†Ô∏è Failed to post to Teams: {response.status_code} - {response.text}")
    except Exception as e:
        print(f"‚ö†Ô∏è Error posting to Teams: {e}")



# After uploading to S3, call this function:
if __name__ == "__main__":
    insights = generate_summary()

    output_key = f"weekly_insights/insight_{datetime.utcnow().strftime('%Y%m%d')}.json"
    s3.put_object(
        Bucket=S3_BUCKET,
        Key=output_key,
        Body=json.dumps({"timestamp": datetime.utcnow().isoformat(), "insights": insights}, indent=2),
        ContentType="application/json"
    )
    print(f"üì¶ Saved to s3://{S3_BUCKET}/{output_key}")

    # Post summary to Teams
    post_to_teams(insights)

